{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e667907-2226-49f1-8854-4eb9840f6561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbb153f-276d-463b-8e9a-fdb581502e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/11/2023-09:10:10] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[08/11/2023-09:10:10] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[08/11/2023-09:10:10] [TRT] [W] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_803/3684369293.py:18: DeprecationWarning: Use network created with NetworkDefinitionCreationFlag::EXPLICIT_BATCH flag instead.\n",
      "  builder.max_batch_size = max_batch_size\n",
      "/tmp/ipykernel_803/3684369293.py:20: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = 1 << 30\n"
     ]
    }
   ],
   "source": [
    "import onnx \n",
    "\n",
    "def load_onnx_model(onnx_model_path):\n",
    "    with open(onnx_model_path, \"rb\") as f: \n",
    "        onnx_model = onnx.load_model(f) \n",
    "        return onnx_model \n",
    "    \n",
    "onnx_model_path = \"rtdetr.onnx\" \n",
    "engine_path=\"detr_3rd.trt\"\n",
    "onnx_model = load_onnx_model(onnx_model_path)\n",
    "\n",
    "def create_engine(onnx_model, max_batch_size, fp16_mode=False): \n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING) \n",
    "    builder = trt.Builder(TRT_LOGGER) \n",
    "    network = builder.create_network(flags=1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) \n",
    "    parser = trt.OnnxParser(network, TRT_LOGGER) \n",
    "    parser.parse(onnx_model.SerializeToString()) \n",
    "    builder.max_batch_size = max_batch_size \n",
    "    config = builder.create_builder_config() \n",
    "    config.max_workspace_size = 1 << 30 \n",
    "    if fp16_mode:\n",
    "        config.set_flag(trt.BuilderFlag.FP16) \n",
    "    engine = builder.build_serialized_network(network, config) \n",
    "    return engine \n",
    "\n",
    "max_batch_size = 1 \n",
    "fp16_mode = False \n",
    "engine = create_engine(onnx_model, max_batch_size, fp16_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d6029b-7f17-41b3-bdcf-2b0dc36cc01b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate file success!\n"
     ]
    }
   ],
   "source": [
    "with open(engine_path, \"wb\") as f:\n",
    "    f.write(engine)\n",
    "    print(\"generate file success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6bcfd58-70ff-44c5-8fbc-0ce869c45309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/11/2023-08:44:10] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    }
   ],
   "source": [
    "f1 = open(engine_path, \"rb\")\n",
    "runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n",
    "\n",
    "engine = runtime.deserialize_cuda_engine(f1.read())\n",
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e8b4c6-b49b-4200-a74a-b20d8f64c441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(frame):\n",
    "    [height, width, _] = frame.shape\n",
    "    length = max((height, width))\n",
    "    image = np.zeros((length, length, 3), np.uint8)\n",
    "    image[0:height, 0:width] = frame\n",
    "    scale = length / 640\n",
    "    blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
    "    return blob,scale\n",
    "\n",
    "image=cv2.imread(\"120.jpg\")\n",
    "img,scale=preprocess(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cca9198-e8cc-46ef-8dec-4eead33a69bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = np.empty([1,5,8400], dtype = np.float32) \n",
    "\n",
    "# allocate device memory\n",
    "d_input = cuda.mem_alloc(1 * img.nbytes)\n",
    "d_output = cuda.mem_alloc(1 * output.nbytes)\n",
    "\n",
    "bindings = [int(d_input), int(d_output)]\n",
    "\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eb812b4-0cf9-4496-bf63-ef534d1861b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(batch): # result gets copied into output\n",
    "    # transfer input data to device\n",
    "    cuda.memcpy_htod_async(d_input, batch, stream)\n",
    "    # execute model\n",
    "    context.execute_async_v2(bindings, stream.handle, None)\n",
    "    # transfer predictions back\n",
    "    cuda.memcpy_dtoh_async(output, d_output, stream)\n",
    "    # syncronize threads\n",
    "    stream.synchronize()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ef2f33-ae14-491b-91b4-201986402b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dddd826-8c31-4098-8c80-ea4817cc50b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.13 ms ± 190 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "pred = predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d3cfa5-eadf-43c1-96b1-188f63f36067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 8400)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13bfcd-3a6e-4990-ac51-3a8182c09b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
